{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_DATAPIPELINE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhyT76Lm0ItY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "8c574d83-0505-4f73-878d-f890fef49085"
      },
      "source": [
        "!pip uninstall tensorflow-addons\n",
        "!pip install tensorflow-addons==0.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-addons-0.8.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/_foo.cpython-36m-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_addons-0.8.3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Collecting tensorflow-addons==0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/ce/ed8472bf2b93b53702f28d91caee52181f7a10bee6eec0617a71dea12fa6/tensorflow_addons-0.10.0-cp36-cp36m-manylinux2010_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.10.0) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_addons"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFGgVsNcZJRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.image import utils as img_utils\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Global Variable to introduce randomness among each element of a batch\n",
        "RANDOM_SEED = tf.random.Generator.from_seed(int(np.random.uniform(low=300, high=9000)))\n",
        "\n",
        "def _angles_to_projective_transforms(angle, image_w, image_h):\n",
        "  \"\"\"Generate projective transform matrix for tfa.image.transform.\n",
        "\n",
        "    Args:\n",
        "      angle(tensorflow.python.framework.ops.EagerTensor): The rotation angle.\n",
        "      image_w(tensorflow.python.framework.ops.EagerTensor): The width of the image.\n",
        "      image_h(tensorflow.python.framework.ops.EagerTensor): The height of the image.\n",
        "\n",
        "    Returns:\n",
        "      projective transform matrix(tensorflow.python.framework.ops.EagerTensor)\n",
        "  \"\"\"\n",
        "  with tf.name_scope(\"rotate_parent\"):\n",
        "    angle_or_angles = tf.convert_to_tensor(angle, name=\"angles\", dtype=tf.dtypes.float32)\n",
        "    angles = angle_or_angles[None]\n",
        "    x_offset = ((image_w - 1) - (tf.math.cos(angles) * (image_w - 1) - tf.math.sin(angles) * (image_h - 1))) / 2.0\n",
        "    y_offset = ((image_h - 1)- (tf.math.sin(angles) * (image_w - 1) + tf.math.cos(angles) * (image_h - 1))) / 2.0\n",
        "    num_angles = tf.shape(angles)[0]\n",
        "  return tf.concat([tf.math.cos(angles)[:, None],-tf.math.sin(angles)[:, None],x_offset[:, None],tf.math.sin(angles)[:, None],tf.math.cos(angles)[:, None],y_offset[:, None],tf.zeros((1, 2))],axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def _rotate(image, angle):\n",
        "  \"\"\"Generates a rotated image with the use of tfa.image.transform\n",
        "\n",
        "    Args:\n",
        "      image(tensorflow.python.framework.ops.Tensor): The image.\n",
        "      angle(tensorflow.python.framework.ops.EagerTensor): The rotation angle.\n",
        "\n",
        "    Returns:\n",
        "      The rotated image.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(\"rotate\"):\n",
        "    image = tf.convert_to_tensor(image)\n",
        "    img = img_utils.to_4D_image(image)\n",
        "    ndim = image.get_shape().ndims\n",
        "    image_h = tf.cast(img.shape[1], tf.dtypes.float32)\n",
        "    image_w = tf.cast(img.shape[2], tf.dtypes.float32)\n",
        "    rotation_key = _angles_to_projective_transforms(angle, image_w, image_h)\n",
        "    output = tfa.image.transform(img, rotation_key, interpolation=\"NEAREST\")\n",
        "  return img_utils.from_4D_image(output, ndim)\n",
        "\n",
        "\n",
        "\n",
        "def _rand_number(low, high):\n",
        "  \"\"\"Generates a random number along a uniform distrubution. \n",
        "\n",
        "    Args:\n",
        "      low(tensorflow.python.framework.ops.Tensor): Minimum Value of the Distrubution.\n",
        "      high(tensorflow.python.framework.ops.EagerTensor): Maximum Value of the Distrubution.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of the specified shape filled with random uniform values.\n",
        "  \"\"\"\n",
        "  # Global Variable defined at the beginning of the file.\n",
        "  global RANDOM_SEED\n",
        "  return RANDOM_SEED.uniform(minval= low, maxval= high, shape = (), dtype=tf.float32)\n",
        "\n",
        "def _preprocessing_selection(choice):\n",
        "  \"\"\"Returns the requested data augmentation function required for the training \n",
        "      specfied.\n",
        "\n",
        "    Args:\n",
        "      choice(str): The type of training the user would like to use.\n",
        "\n",
        "    Returns:\n",
        "      function: A function for data augmentation for the specfic training specified.\n",
        "  \"\"\"\n",
        "\n",
        "  def classification(datapoint):\n",
        "    \"\"\"Augments image by performing Random Zoom, Resize with Pad, Random Rotate, \n",
        "      Random Brightness Distortion, Random Saturation Distortion, Random Hue Distortion \n",
        "      and finally normalizing the image. \n",
        "\n",
        "      Args:\n",
        "        datapoint (dict): A Dictionaty that holds the image as well as other relevant \n",
        "          information.\n",
        "\n",
        "      Returns:\n",
        "        Either Image and Label or Image and Object.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generates Random Variables that will be used within the Data Augmentation Function.\n",
        "    image = datapoint['image']\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    w = tf.cast(image.shape[1], tf.float32)\n",
        "    h = tf.cast(image.shape[2], tf.int32)\n",
        "    low = tf.cast(128, tf.dtypes.float32)[None]\n",
        "    high = tf.cast(448, tf.dtypes.float32)[None]\n",
        "    scale = tf.py_function(_rand_number, [low, high], [tf.float32])\n",
        "    aspect = tf.py_function(_rand_number, [.75, 1.25], [tf.float32])\n",
        "    deg = tf.py_function(_rand_number, [-7.0, 7.0], [tf.float32])\n",
        "    scale = tf.cast(scale, dtype= tf.int32)[0][0]\n",
        "    deg = tf.cast(deg, dtype=tf.float32)[0]\n",
        "    aspect = tf.cast(aspect, dtype=tf.float32)[0]\n",
        "    nh = tf.cast(w/aspect, dtype= tf.int32)\n",
        "    nw = tf.cast(w, dtype= tf.int32)\n",
        "\n",
        "    # Data Augmentation Functions.\n",
        "    image = tf.image.resize(image, size = (nw, nh))\n",
        "    image = tf.image.resize_with_crop_or_pad(image, target_height = scale, target_width = scale) # Zoom\n",
        "    image = tf.image.resize_with_pad(image, target_width=224, target_height=224) # Final Output Shape\n",
        "    image = _rotate(image, deg) # Rotate\n",
        "    image = tf.image.random_brightness(image=image, max_delta=.75) # Brightness\n",
        "    image = tf.image.random_saturation(image=image, lower = 0.75, upper=1.25) # Saturation\n",
        "    image = tf.image.random_hue(image=image, max_delta=.1) # Hue\n",
        "    image = tf.clip_by_value(image / 255, 0, 1) # Normalize\n",
        "\n",
        "    # Return\n",
        "    if \"object\" in datapoint:\n",
        "      return image, datapoint['object']\n",
        "    else:\n",
        "      return image, datapoint['label']\n",
        "\n",
        "  def priming(datapoint):\n",
        "    \"\"\"Augments image by performing Random Zoom, Resize with Pad, and \n",
        "        finally normalizing the image. \n",
        "\n",
        "      Args:\n",
        "        datapoint (dict): A Dictionaty that holds the image as well as other relevant \n",
        "          information.\n",
        "\n",
        "      Returns:\n",
        "        Either Image and Label or Image and Object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generates Random Variables that will be used within the Data Augmentation Function.\n",
        "    image = datapoint['image']\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    w = tf.cast(image.shape[1], tf.float32)\n",
        "    h = tf.cast(image.shape[2], tf.int32)\n",
        "    low = tf.cast(448, tf.dtypes.float32)[None]\n",
        "    high = tf.cast(512, tf.dtypes.float32)[None]\n",
        "    scale = tf.py_function(_rand_number, [low, high], [tf.float32])\n",
        "    scale = tf.cast(scale, dtype= tf.int32)[0][0]\n",
        "\n",
        "    # Data Augmentation Functions.\n",
        "    image = tf.image.resize_with_crop_or_pad(image, target_height = scale, target_width = scale) # Zoom\n",
        "    image = tf.image.resize_with_pad(image, target_width=448, target_height=448) # Final Output Shape\n",
        "    image = image / 255 #Normalize\n",
        "\n",
        "    # Return\n",
        "    if \"object\" in datapoint:\n",
        "      return image, datapoint['object']\n",
        "    else:\n",
        "      return image, datapoint['label']\n",
        "\n",
        "  def detection(datapoint):\n",
        "    \"\"\"Augments image by performing Random Resize with Pad, Random Brightness Distortion, \n",
        "      Random Saturation Distortion, Random Hue Distortion and finally normalizing the image.  \n",
        "\n",
        "      Args:\n",
        "        datapoint (dict): A Dictionaty that holds the image as well as other relevant \n",
        "          information.\n",
        "\n",
        "      Returns:\n",
        "        Either Image and Label or Image and Object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generates Random Variables that will be used within the Data Augmentation Function.\n",
        "    image = datapoint['image']\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    low = tf.cast(128, tf.dtypes.float32)[None]\n",
        "    high = tf.cast(448, tf.dtypes.float32)[None]\n",
        "    resize_num = tf.py_function(_rand_number, [10.0, 19.0], [tf.float32])\n",
        "    resize_num = tf.cast(resize_num, dtype= tf.int32)[0]*32\n",
        "\n",
        "    # Data Augmentation Functions.\n",
        "    image = tf.image.resize_with_pad(image, target_width=resize_num, target_h=resize_num) # Random Resize\n",
        "    image = tf.image.random_brightness(image=image, max_delta=.75) # Brightness \n",
        "    image = tf.image.random_saturation(image=image, lower = 0.75, upper=1.25) # Saturation\n",
        "    image = tf.image.random_hue(image=image, max_delta=.1) # Hue\n",
        "    image = image / 255 # Normalize\n",
        "\n",
        "    # Return\n",
        "    if \"object\" in datapoint:\n",
        "      return image, datapoint['object']\n",
        "    else:\n",
        "      return image, datapoint['label']\n",
        "    \n",
        "  if choice.lower() == \"detection\":\n",
        "    return detection\n",
        "  elif choice.lower() == \"classification\":\n",
        "    return classification\n",
        "  elif choice.lower() == \"priming\":\n",
        "    return priming\n",
        "    \n",
        "def _normalize_selection(h, w):\n",
        "  \"\"\"Returns the requested normalization function required for the width and height \n",
        "      specified\n",
        "\n",
        "    Args:\n",
        "      h (int): Height of desired output image.\n",
        "      w (int): Width of desired output image.\n",
        "\n",
        "    Returns:\n",
        "      function: A function for normalize for the specfic training specified.\n",
        "  \"\"\"\n",
        "  def normalize(datapoint):\n",
        "    \"\"\"Normalizes the image by resizing it to the desired output shape\n",
        "\n",
        "      Args:\n",
        "        datapoint (dict): A Dictionaty that holds the image as well as other relevant \n",
        "          information.\n",
        "\n",
        "      Returns:\n",
        "        normalize (dict): A Normalized Image alongside the mapped information.\n",
        "    \"\"\"\n",
        "    image = datapoint['image']\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    # Normalization Functions.\n",
        "    image = tf.image.resize_with_pad(image, target_width=h, target_height=w) # Final Output Shape\n",
        "    image = image / 255 # Normalize\n",
        "\n",
        "    # Return\n",
        "    if \"object\" in datapoint:\n",
        "      return image, datapoint['object']\n",
        "    else:\n",
        "      return image, datapoint['label']\n",
        "  return normalize\n",
        "\n",
        "def _detection_normalize(datapoint):\n",
        "  \"\"\"Normalizes the image by doing random resizing required for detection.\n",
        "\n",
        "    Args:\n",
        "      datapoint (dict): A Dictionaty that holds the image as well as other relevant \n",
        "        information.\n",
        "\n",
        "    Returns:\n",
        "      normalize (dict): A Normalized Image alongside the mapped information.\n",
        "  \"\"\"\n",
        "  # Generates Random Variables that will be used within the Normalization Function.\n",
        "  image = datapoint['image']\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  low = tf.cast(128, tf.dtypes.float32)[None]\n",
        "  high = tf.cast(448, tf.dtypes.float32)[None]\n",
        "  resize_num = tf.py_function(_rand_number, [10.0, 19.0], [tf.float32])\n",
        "  resize_num = tf.cast(resize_num, dtype= tf.int32)[0]*32\n",
        "\n",
        "  # Normalization Functions.\n",
        "  image = tf.image.resize_with_pad(image, target_width=resize_num, target_h=resize_num) # Final Output Shape\n",
        "  image = image / 255 # Normalize\n",
        "\n",
        "  # Return\n",
        "  if \"object\" in datapoint:\n",
        "    return image, datapoint['object']\n",
        "  else:\n",
        "    return image, datapoint['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCrf1d00j8ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "eb561a15-e2e2-488a-bac1-0513b6430467"
      },
      "source": [
        "#param train, top%, bottom%, type Splits before preprocessing_type\n",
        "def preprocessing(dataset, data_augmentation_split, preprocessing_type, size, num_of_batches):\n",
        "  \"\"\"Preprocesses (normalization and data augmentation) and batches the dataset.\n",
        "\n",
        "    Args:\n",
        "      dataset (tfds.data.Dataset): The Dataset you would like to preprocess.\n",
        "      data_augmentation_split (int): The percentage of the dataset that is data \n",
        "        augmented.\n",
        "      preprocessing_type (str): The type of preprocessing should be conducted \n",
        "        and is dependent on the type of training.\n",
        "      size (int): The size of the dataset being passed into preprocessing.\n",
        "      num_of_batches (int): The number of batches you would like the return\n",
        "        dataset to be split into. \n",
        "\n",
        "    Returns:\n",
        "      dataset (tfds.data.Dataset): A shuffled dataset that includes images that\n",
        "        have been data augmented \n",
        "\n",
        "    Raises:\n",
        "      SyntaxError: \n",
        "        - Preprocessing type not found.\n",
        "        - The given batch number for detection preprocessing is more than 1.\n",
        "        - Number of batches cannot be less than 1.\n",
        "        - Data augmentation split cannot be greater than 100.\n",
        "      TypeError:\n",
        "        - Dataset is not a tensorflow dataset.\n",
        "        - Data augmentation split must be an integer.\n",
        "        - Preprocessing type must be an string.\n",
        "        - Size must be an integer.\n",
        "        - Number of batches must be an integer.\n",
        "  \"\"\"\n",
        "  if isinstance(dataset, tf.python.data.ops.dataset_ops.DatasetV1Adapter) == False:\n",
        "    raise TypeError(\"Dataset is not a tensorflow dataset.\")\n",
        "  if type(data_augmentation_split) is not int:\n",
        "    raise TypeError(\"Data augmentation split must be an integer.\")\n",
        "  if type(preprocessing_type) is not str:\n",
        "    raise TypeError(\"Preprocessing type must be an string.\")\n",
        "  if type(size) is not int:\n",
        "    raise TypeError(\"Size must be an integer.\")\n",
        "  if type(num_of_batches) is not int:\n",
        "    raise TypeError(\"Number of batches must be an integer.\")\n",
        "\n",
        "  if preprocessing_type.lower() != \"detection\" and preprocessing_type.lower() != \"classification\" and preprocessing_type.lower() != \"priming\":\n",
        "    raise SyntaxError(\"Preprocessing type not found.\")\n",
        "  if num_of_batches != 1 and preprocessing_type.lower() == \"detection\":\n",
        "    raise SyntaxError(\"For detection preprocessing, number of batches must be 1.\")\n",
        "  if num_of_batches < 1:\n",
        "    raise SyntaxError(\"Number of batches cannot be less than 1.\")\n",
        "  if data_augmentation_split > 100:\n",
        "    raise SyntaxError(\"Data augmentation split cannot be greater than 100.\")\n",
        "\n",
        "  # Spliting the dataset based off of user defined split\n",
        "  data_augmentation_split = int((data_augmentation_split/100)*size)\n",
        "  non_preprocessed_split = size - data_augmentation_split\n",
        "  data_augmentation_dataset = dataset.take(data_augmentation_split)\n",
        "  remaining = dataset.skip(data_augmentation_split)  \n",
        "  non_preprocessed_split = remaining.take(non_preprocessed_split)\n",
        "\n",
        "  # Data Augmentation\n",
        "  preprocessing_function = _preprocessing_selection(preprocessing_type)\n",
        "  data_augmentation_dataset = data_augmentation_dataset.map(preprocessing_function, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Normalization\n",
        "  if preprocessing_type.lower() == \"detection\":\n",
        "    non_preprocessed_split = non_preprocessed_split.map(_detection_normalize, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  elif preprocessing_type.lower() == \"classification\":\n",
        "    normalize = _normalize_selection(224, 224)\n",
        "    non_preprocessed_split = non_preprocessed_split.map(normalize, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  elif preprocessing_type.lower() == \"priming\":\n",
        "    normalize = _normalize_selection(448, 448)\n",
        "    non_preprocessed_split = non_preprocessed_split.map(normalize, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Preparing the return dataset through concatentaion and shuffling\n",
        "  dataset= data_augmentation_dataset.concatenate(non_preprocessed_split)\n",
        "  dataset = dataset.shuffle(size)\n",
        "  dataset = dataset.batch(int(size/num_of_batches)).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "Train, Info = tfds.load('rock_paper_scissors', split='train', with_info=True, shuffle_files=False)\n",
        "Size = int(Info.splits['train'].num_examples)\n",
        "Train = preprocessing(Train, 50,\"classification\", Size, 30) # https://www.calculatorsoup.com/calculators/math/factors.php -> Check to find a good batch size\n",
        "\n",
        "#benchmark\n",
        "start = time.time()\n",
        "count = 0\n",
        "for x in range(30):\n",
        "  train_ds = Train.take(1)\n",
        "  print(count)\n",
        "  count += 1\n",
        "stop = time.time()\n",
        "print(stop - start)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "0.013044118881225586\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}